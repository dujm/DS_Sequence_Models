
####  [Home](https://dujm.github.io/)

## Word embeddings
Word embedding is one of the most popular word representation It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.

### 1. What are word representations?
  * One-Hot encodings
  * Featurized representaion: word embeddings

### 2. What is word embeddings?  

### 3. Why do we use word embeddings?


### 4. Summary
|   | Word Embeddings   |  
|---|---|
| Transfer Learning |  yes    |
| Training Set |  small  |  
|Entity recognition, text summarization, co-reference resolution, parsing   |Good  |   
| Language modeling and machine translation    | Bad |   
| Visulization   | t-SNE|    
| Properties of word embeddings  |  yes  |  

### 5. How does word embeddings algorithms work?
Take a look at [Coursera notebook]{https://github.com/dujm/DS_Sequence_Models/blob/master/notebooks/Finished/w2_Operations%2Bon%2Bword%2Bvectors%2B-%2Bv2_DJ.ipynb}

Or my otebook{https://github.com/dujm/DS_Sequence_Models/tree/master/notebooks/Finished}
(Pictures need to be implemented)




--

### Reference
[Natural Language Processing & Word Embeddings | Coursera](https://www.coursera.org/learn/nlp-sequence-models/home/week/2)

[Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)
